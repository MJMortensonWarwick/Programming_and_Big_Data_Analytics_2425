{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO1ZvCYLiACsi9I5fnH/SQV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MJMortensonWarwick/Programming_and_Big_Data_Analytics_2425/blob/main/8_01_convolutional_neural_networks_part_ii.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1vv_PsWBnUJwSCkwKDoJAC-vXjtaEA4Ts)"
      ],
      "metadata": {
        "id": "ii34M4muJPEU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.01 Convolutional Neural Networks (part II)\n",
        "In this tutorial you will build your own CNN in PyTorch! Please reference back to Notebook 7.02 for insights on to how the code works!\n",
        "\n",
        "We will be working with a chest x-ray set from Kaggle ([here](https://www.kaggle.com/datasets/sayakbera/fer-2013-7-emotions-uniform-dataset)) where we are predicting the emotion of people in different images.\n",
        "\n",
        "To get the data we need an API key from Kaggle. To set up an account (if you haven't got one already):\n",
        "* Click on your avatar in the top right and \"Account\" from the dropdown menu.\n",
        "* Select \"Settings\"\n",
        "* Scroll down the page and you'll find a button \"Create Legacy API Key\".\n",
        "* This downloads an API key to your PC which you can upload here.\n",
        "* Make sure you create a _Legacy_ key not a new one!!!!"
      ],
      "metadata": {
        "id": "X6oownATtXqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "LHG_Y_YFFeEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we need to move this to a folder called kaggle as this is where Colab will look to find the API key. This has been written as Linux commands as we are doing this directly on the underlying (virtual) machine not within Python. Linux commands start with a \"!\" in Colab. Linux is out of scope for the module so we won't go into detail on the code."
      ],
      "metadata": {
        "id": "iBOOZx3Kuey-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "pU043sJiFjet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Extraction\n",
        "Now you can download the data. The following can be used for any Kaggle dataset. When you are on the dataset page click \"Download\" and \"Download via CLI\". This gives you code that looks like below (but add a \"!\" at the start):"
      ],
      "metadata": {
        "id": "EPsEVPNyuoo4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "048YVK6TChZV"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d sayakbera/fer-2013-7-emotions-uniform-dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This has added the data as a zip folder. We can unzip it and create a new folder (Linux again):"
      ],
      "metadata": {
        "id": "yddDe7JWvFEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip fer-2013-7-emotions-uniform-dataset.zip -d facial-emotion"
      ],
      "metadata": {
        "id": "pf7pLBawF3e0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Testn Split\n",
        "This data comes in three sets, `train`, `val` (validation) and `test`. We have seen two of these before, but `val` is new.\n",
        "\n",
        "Because deep learning effectively uses _epochs_ as a hyperparameter, and the length of time we train the model will impact under- and over-fitting, we want to measure performance every $x$ number of epochs. As with hyperparameter tuning in the classical space, we don't want to use `test` to check perofmance as this is cheating and can cause _data leakage_ (the model being able to see the test data). We also don't want to use the training data as this can't measure overfitting (overfitting can only be measured on unseen data). We could do cross validation but this will be very expensive to do over many epochs (i.e. it will take ages). So instead we will split our data an extra time and create a `val` set. This will be used to measure performance over epochs, but also can be used to tune other hyperparameters.\n",
        "\n",
        "For ease we will create some variables to store the path to the directories:"
      ],
      "metadata": {
        "id": "NSE9XpymvNeS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = \"/content/facial-emotion/FER2013_7emotions_Uniform_Augmented_Dataset/train\"\n",
        "test_dir = \"/content/facial-emotion/FER2013_7emotions_Uniform_Augmented_Dataset/test\"\n",
        "val_dir = \"/content/facial-emotion/FER2013_7emotions_Uniform_Augmented_Dataset/validation\""
      ],
      "metadata": {
        "id": "yOQLobhOZsI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation\n",
        "Now we can add a generator to prepare out data. This data will perform basic cleaning operations on the data and feed it into the model.\n",
        "\n",
        "The three tasks we will do are:\n",
        "* `ToTensor()` changes the data into tensor format as before.\n",
        "* `Resize()` changes the size of the images to something smaller (to make computations faster and easier).\n",
        "* `Normalize()` (sic) effectively normalises the data as we have done previously with min-max or standardisation.\n",
        "\n",
        "For images normalisation is slighltly different as we are working with RGB pixels, meaning each data item is a red score between 0 (no red) and 255 (red); a green score between 0 (no green) and 255 (green); and a blue score between 0 (no blue) and 255 (blue). This means if a pixel is stored as:\n",
        "* RGB = [0,255,0] the pixel colour will be green.\n",
        "* RGB = [255,0,0] the colour will be red.\n",
        "* RGB = [0,0,0] the colour will be white.\n",
        "* RGB = [255,255,255] the colour will be black.\n",
        "* RGB = [255,0,255] the colour will be purple.\n",
        "* And so on.\n",
        "\n",
        "Because we are not scaling based on mean or max values in our data, all our data has the same scale, we apply the same transformation to all the datasets.\n",
        "\n",
        "We do these transformations for each of our datasets (`train`, `val` and `test`) and then create a `DataLoader` to move the data into the model. First we split into batches of 64 items per batch. We _shuffle_ our training data, meaning we put the data into random order in case there are biases in how it has been saved that would then be present in our batches. `pin_memory` is used just to make things a little faster as the Colab environment is slightly underpowered.\n",
        "\n",
        "However, we might also want to experiment with data augmentations! You will see some commented out code, and instructions on the hyperparameters. What should be added? What is appropriate for the problem space?"
      ],
      "metadata": {
        "id": "xbsVBVhIvSom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define data transformations\n",
        "train_transform = transforms.Compose([\n",
        "    #transforms.RandomResizedCrop(224), # Randomly crop and resize\n",
        "    #transforms.RandomHorizontalFlip(),  # Example augmentation\n",
        "    #transforms.RandomVerticalFlip(),  # Example augmentation\n",
        "    #transforms.RandomRotation(n),  # Example augmentation change n to max degrees you want to rotate\n",
        "    #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1), # Random color adjustments\n",
        "\n",
        "    # keep these ones in!\n",
        "    transforms.ToTensor(),  # Convert to tensor\n",
        "    transforms.Resize((32, 32)),  # Resize images (adjust size as needed)\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # ImageNet normalization\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load datasets and apply transfomrations\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
        "val_dataset = datasets.ImageFolder(root=val_dir, transform=val_transform)\n",
        "test_dataset = datasets.ImageFolder(root=test_dir, transform=test_transform)\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 128  # Adjust batch size as needed (hyperparameter!)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)"
      ],
      "metadata": {
        "id": "dFf8B5smp1MU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolutional Neural Network\n",
        "Now we can build our model.\n",
        "\n",
        "You will have to decide what layers you think are appropriate! You can see the code involved in Notebook 7.02.\n",
        "\n",
        "With each layer ... e.g. `self.conv1(x, y, kernel_size=3)` ... you need to specify _x_ as the number of neurons in the previous layer and _y_ as the size of the output. If we have the above convolutional layer as the first layer, _x_ will likely be 3 (RGB colours) and _y_ will be the number of features. The next layer's input will then need to be the same as the previous ... e.g. `nn.BatchNorm2d(32)` (normalisation has the same output size as input size so only one number). In some cases its hard to calculate what the number should be. You can use AI to help.\n",
        "\n",
        "Remember, the optimal number of filters (etc.) is very problem specific. What do you think it should be?"
      ],
      "metadata": {
        "id": "8S2rBMlQvpHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Based on the dataset \"facial-emotion\", there are 7 classes.\n",
        "num_classes = 7\n",
        "\n",
        "class EmotionCNN(nn.Module):\n",
        "    def __init__(self, num_classes=8):\n",
        "        super(EmotionCNN, self).__init__()\n",
        "        # e.g. self.conv1 = nn.Conv2d(3, [...], kernel_size=[...])\n",
        "        # e.g. self.bn1 = nn.BatchNorm2d([...])\n",
        "        # [...] # more layers\n",
        "        # e.g. self.dropout = nn.Dropout(0.25)\n",
        "        self.fc2 = nn.Linear([...], num_classes) # Specify the size of the previous layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Should match the above\n",
        "        # e.g. x = F.relu(self.conv1(x))\n",
        "        # [...] # more layers\n",
        "        return self.fc2(x) # Removed sigmoid for multi-class with CrossEntropyLoss\n",
        "\n",
        "\n",
        "cnn = EmotionCNN()\n",
        "cnn"
      ],
      "metadata": {
        "id": "jT9g_eoUq07y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To complete our model let's specify the loss function and optimiser. We will use a slightly different optimiser than before - [Adam](https://arxiv.org/abs/1412.6980). You can read more from the link if you want to. We will also start with a learning rate of 0.001 (0.1%) and will reduce this number by 0.0005 every epoch. This means that as the training goes on we will take smaller and smaller steps so we can be sure we don't miss the optimal model (the one with the lowest loss). You certainly can experiment with the learning rate (`lr`) and the decay rate (`weight_decay`)."
      ],
      "metadata": {
        "id": "NS36RAtXbamM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss function and optimiser\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() # Cross-Entropy Loss for multi-class classification\n",
        "optimiser = optim.Adam(cnn.parameters(), lr=0.001, weight_decay=0.0005) # Adam optimizer\n",
        "# we add a weight decay parameter to reduce the learning rate each epoch"
      ],
      "metadata": {
        "id": "X4ZbqKGsK3yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how many parameters this is:"
      ],
      "metadata": {
        "id": "P8Lmr1pVr5uS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model to device\n",
        "from torchsummary import summary\n",
        "\n",
        "# Move data to GPU to get summary\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "cnn = cnn.to(device)\n",
        "\n",
        "summary(cnn, (3, 32, 32))"
      ],
      "metadata": {
        "id": "_3-C4ULMr_em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to apply _early stopping_, which is a new idea. Because we might overfit by running too many epochs, we will set up to autoamtically stop if the performance on validation doesn't improve after 5 epochs (the number 5 is effectively a hyperparameter we will call our _patience_). I.e. if epoch #6 give us a our best score, we then have until epoch #11 to get a better score or the training automatically stops."
      ],
      "metadata": {
        "id": "Agm_F6Ievs3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 5\n",
        "best_val_loss = float('inf')\n",
        "epochs_no_improve = 0\n",
        "\n",
        "# empty lists to store losses for visualisation at the end\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 500\n",
        "\n",
        "# For loop for training\n",
        "for epoch in range(num_epochs):\n",
        "    cnn.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        # add images and label in batch to the GPU\n",
        "        images = images.to(device)\n",
        "        # Labels should be LongTensor for CrossEntropyLoss and no unsqueeze(1)\n",
        "        labels = labels.long().to(device) # Explicitly cast to LongTensor\n",
        "\n",
        "        cnn = cnn.to(device) # Pass model to GPU as well (already moved above, but doesn't hurt)\n",
        "\n",
        "        optimiser.zero_grad() # reset the optimiser to start optimisation again\n",
        "\n",
        "        # run the images through the model\n",
        "        outputs = cnn(images)\n",
        "        loss = criterion(outputs, labels) # calculate loss\n",
        "        loss.backward() # backpropogate\n",
        "        optimiser.step() # update parameters\n",
        "        running_loss += loss.item() # keep track of loss for visualisation\n",
        "\n",
        "    # store losses for visualisation\n",
        "    train_losses.append(running_loss/len(train_loader)) # average loss\n",
        "\n",
        "    # Validation\n",
        "    cnn.eval()\n",
        "    val_loss = 0.0\n",
        "    all_preds = [] # holder for predictions as 0 or 1\n",
        "    all_labels = [] # compare with labels\n",
        "    with torch.no_grad(): # not update/backpropogation\n",
        "      # Warning: val_loader is not defined in the current notebook setup.\n",
        "      # Temporarily using test_loader for validation. Ideally, a proper val_dataset and val_loader should be created.\n",
        "      if 'val_loader' not in locals() and 'val_loader' not in globals():\n",
        "          print(\"Warning: val_loader is not defined. Using test_loader for validation.\")\n",
        "          current_val_loader = test_loader\n",
        "      else:\n",
        "          current_val_loader = val_loader\n",
        "\n",
        "      for images, labels in current_val_loader:\n",
        "          # add images and label in batch to the GPU\n",
        "          images = images.to(device)\n",
        "          # Labels should be LongTensor for CrossEntropyLoss and no unsqueeze(1)\n",
        "          labels = labels.long().to(device) # Explicitly cast to LongTensor\n",
        "          cnn = cnn.to(device) # Pass model as well\n",
        "\n",
        "          # run the images through the model\n",
        "          outputs = cnn(images)\n",
        "          loss = criterion(outputs, labels) # calculate loss\n",
        "          val_loss += loss.item() # keep track of loss for visualisation\n",
        "\n",
        "    # store losses and recall for visualisation\n",
        "    val_losses.append(val_loss/len(current_val_loader)) # average loss\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss < best_val_loss: # is this better than previous record?\n",
        "        best_val_loss = val_loss # if so set as new best\n",
        "        epochs_no_improve = 0 # make the no improve counter zero again\n",
        "    else:\n",
        "        epochs_no_improve += 1 # otherwise increase the count by one\n",
        "\n",
        "    train_loss_rnd = round(running_loss / len(train_loader), 4) # avg train loss\n",
        "    val_loss_rnd = round(val_loss / len(current_val_loader), 4) # avg val loss\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_loss_rnd}, Validation Loss: {val_loss_rnd}\")\n",
        "\n",
        "\n",
        "    if epochs_no_improve == patience: # if epochs without improvement is at the patience level\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break"
      ],
      "metadata": {
        "id": "bRu41LfNtyEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "Let's visualise the results:"
      ],
      "metadata": {
        "id": "4Stqkqz8vyaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JCedkXjKcJJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see loss on the training set has steadily improved. On the validation set its a bit more \"bouncey\" but then that's not a big set of images. It looks like our best performance was around 5 epochs. (Note, your results may change if you run it due to how we randomly split the data and so on). Let's see how we do on test:"
      ],
      "metadata": {
        "id": "d3PkH6Mxv12T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on the test set\n",
        "from sklearn.metrics import recall_score, accuracy_score\n",
        "\n",
        "cnn.eval()\n",
        "test_loss = 0.0\n",
        "all_preds = []  # Store all predictions\n",
        "all_true_labels = []  # Store all true labels\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader: # Using val_loader for evaluation, consistent with training loop\n",
        "        images = images.to(device)\n",
        "        # Labels should be LongTensor and 1D for CrossEntropyLoss\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = cnn(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        # For multi-class, predictions are the class with the highest logit\n",
        "        predicted_labels = torch.argmax(outputs, dim=1)\n",
        "\n",
        "        all_preds.extend(predicted_labels.cpu().numpy())\n",
        "        all_true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "test_loss_rnd = round(test_loss / len(val_loader), 4)\n",
        "\n",
        "# Calculate metrics for multi-class\n",
        "test_accuracy = accuracy_score(all_true_labels, all_preds)\n",
        "# 'weighted' accounts for class imbalance\n",
        "test_recall = recall_score(all_true_labels, all_preds, average='weighted', zero_division=0)\n",
        "\n",
        "\n",
        "print(f\"Test Loss: {test_loss_rnd}\")\n",
        "print(f\"Test Accuracy: {round(test_accuracy, 4)}\")\n",
        "print(f\"Test Recall (weighted): {round(test_recall, 4)}\")"
      ],
      "metadata": {
        "id": "Fv-jJ8sJcRBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How did you do? What could improve it?"
      ],
      "metadata": {
        "id": "UXoMUScAwBdy"
      }
    }
  ]
}